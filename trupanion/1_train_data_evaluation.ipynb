{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PolicyId</th>\n",
       "      <th>EnrollDate</th>\n",
       "      <th>CancelDate</th>\n",
       "      <th>MonthlyPremium</th>\n",
       "      <th>ClaimedAmount</th>\n",
       "      <th>PaidAmount</th>\n",
       "      <th>CustomerPaidAmount</th>\n",
       "      <th>ClaimsCount</th>\n",
       "      <th>AvgClaimAmount</th>\n",
       "      <th>AvgPaidAmount</th>\n",
       "      <th>...</th>\n",
       "      <th>VarianceCustomerPaidAmount</th>\n",
       "      <th>maxClaimAmount</th>\n",
       "      <th>minClaimAmount</th>\n",
       "      <th>medianClaimAmount</th>\n",
       "      <th>maxCustomerPaidAmount</th>\n",
       "      <th>minCustomerPaidAmount</th>\n",
       "      <th>medianCustomerPaidAmount</th>\n",
       "      <th>LastSeenDate</th>\n",
       "      <th>LOS</th>\n",
       "      <th>InsuranceStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93090</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>23.53</td>\n",
       "      <td>762.49</td>\n",
       "      <td>329.46</td>\n",
       "      <td>433.03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>254.16</td>\n",
       "      <td>109.82</td>\n",
       "      <td>...</td>\n",
       "      <td>7971.91</td>\n",
       "      <td>510.48</td>\n",
       "      <td>100.00</td>\n",
       "      <td>152.01</td>\n",
       "      <td>510.48</td>\n",
       "      <td>100.00</td>\n",
       "      <td>152.01</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>73.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93258</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>21.66</td>\n",
       "      <td>33.99</td>\n",
       "      <td>30.60</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.33</td>\n",
       "      <td>10.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.33</td>\n",
       "      <td>11.33</td>\n",
       "      <td>11.33</td>\n",
       "      <td>11.33</td>\n",
       "      <td>11.33</td>\n",
       "      <td>11.33</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>66.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1324</td>\n",
       "      <td>2009-05-11</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>38.84</td>\n",
       "      <td>754.76</td>\n",
       "      <td>315.10</td>\n",
       "      <td>439.66</td>\n",
       "      <td>7.0</td>\n",
       "      <td>107.82</td>\n",
       "      <td>45.01</td>\n",
       "      <td>...</td>\n",
       "      <td>3215.09</td>\n",
       "      <td>363.81</td>\n",
       "      <td>15.23</td>\n",
       "      <td>89.88</td>\n",
       "      <td>363.81</td>\n",
       "      <td>15.23</td>\n",
       "      <td>89.88</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>91.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1406</td>\n",
       "      <td>2009-02-20</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>41.44</td>\n",
       "      <td>1518.46</td>\n",
       "      <td>1095.27</td>\n",
       "      <td>423.19</td>\n",
       "      <td>8.0</td>\n",
       "      <td>189.81</td>\n",
       "      <td>136.91</td>\n",
       "      <td>...</td>\n",
       "      <td>2545.23</td>\n",
       "      <td>777.98</td>\n",
       "      <td>30.95</td>\n",
       "      <td>97.62</td>\n",
       "      <td>777.98</td>\n",
       "      <td>30.95</td>\n",
       "      <td>97.62</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>94.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1413</td>\n",
       "      <td>2009-02-23</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>21.24</td>\n",
       "      <td>336.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>336.81</td>\n",
       "      <td>5.0</td>\n",
       "      <td>67.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1740.78</td>\n",
       "      <td>134.76</td>\n",
       "      <td>35.06</td>\n",
       "      <td>46.23</td>\n",
       "      <td>134.76</td>\n",
       "      <td>35.06</td>\n",
       "      <td>46.23</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>94.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1499</td>\n",
       "      <td>2009-05-29</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>20.53</td>\n",
       "      <td>1801.90</td>\n",
       "      <td>1355.14</td>\n",
       "      <td>446.76</td>\n",
       "      <td>2.0</td>\n",
       "      <td>900.95</td>\n",
       "      <td>677.57</td>\n",
       "      <td>...</td>\n",
       "      <td>47345.80</td>\n",
       "      <td>1732.38</td>\n",
       "      <td>69.52</td>\n",
       "      <td>900.95</td>\n",
       "      <td>1732.38</td>\n",
       "      <td>69.52</td>\n",
       "      <td>900.95</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>91.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PolicyId EnrollDate CancelDate  MonthlyPremium  ClaimedAmount  PaidAmount  \\\n",
       "2      93090 2010-11-11 1970-01-01           23.53         762.49      329.46   \n",
       "5      93258 2010-11-11 2016-05-24           21.66          33.99       30.60   \n",
       "7       1324 2009-05-11 1970-01-01           38.84         754.76      315.10   \n",
       "9       1406 2009-02-20 1970-01-01           41.44        1518.46     1095.27   \n",
       "10      1413 2009-02-23 1970-01-01           21.24         336.81        0.00   \n",
       "13      1499 2009-05-29 1970-01-01           20.53        1801.90     1355.14   \n",
       "\n",
       "    CustomerPaidAmount  ClaimsCount  AvgClaimAmount  AvgPaidAmount  \\\n",
       "2               433.03          3.0          254.16         109.82   \n",
       "5                 3.39          3.0           11.33          10.20   \n",
       "7               439.66          7.0          107.82          45.01   \n",
       "9               423.19          8.0          189.81         136.91   \n",
       "10              336.81          5.0           67.36           0.00   \n",
       "13              446.76          2.0          900.95         677.57   \n",
       "\n",
       "         ...         VarianceCustomerPaidAmount  maxClaimAmount  \\\n",
       "2        ...                            7971.91          510.48   \n",
       "5        ...                               0.00           11.33   \n",
       "7        ...                            3215.09          363.81   \n",
       "9        ...                            2545.23          777.98   \n",
       "10       ...                            1740.78          134.76   \n",
       "13       ...                           47345.80         1732.38   \n",
       "\n",
       "    minClaimAmount  medianClaimAmount  maxCustomerPaidAmount  \\\n",
       "2           100.00             152.01                 510.48   \n",
       "5            11.33              11.33                  11.33   \n",
       "7            15.23              89.88                 363.81   \n",
       "9            30.95              97.62                 777.98   \n",
       "10           35.06              46.23                 134.76   \n",
       "13           69.52             900.95                1732.38   \n",
       "\n",
       "    minCustomerPaidAmount  medianCustomerPaidAmount  LastSeenDate    LOS  \\\n",
       "2                  100.00                    152.01    2016-12-31  73.66   \n",
       "5                   11.33                     11.33    2016-05-24  66.40   \n",
       "7                   15.23                     89.88    2016-12-31  91.70   \n",
       "9                   30.95                     97.62    2016-12-31  94.33   \n",
       "10                  35.06                     46.23    2016-12-31  94.23   \n",
       "13                  69.52                    900.95    2016-12-31  91.11   \n",
       "\n",
       "    InsuranceStatus  \n",
       "2                 1  \n",
       "5                 0  \n",
       "7                 1  \n",
       "9                 1  \n",
       "10                1  \n",
       "13                1  \n",
       "\n",
       "[6 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read data\n",
    "claim_df =  pd.read_csv(\"./data/ClaimLevel.csv\")\n",
    "policy_df = pd.read_csv(\"./data/PolicyLevel.csv\")\n",
    "\n",
    "\"\"\"\n",
    "                        # Data Preparation & Feature generation#\n",
    "Generate a single record per policy that retains max information possible\n",
    "\"\"\"\n",
    "claim_df['CustomerPaidAmount'] = claim_df['ClaimedAmount'] - claim_df['PaidAmount']\n",
    "claim_df_grouped = claim_df.groupby(['PolicyId'])\n",
    "claim_per_policy_df = claim_df_grouped[['ClaimedAmount', 'PaidAmount', 'CustomerPaidAmount']].sum()\n",
    "claim_per_policy_df['ClaimsCount'] = claim_df_grouped.size()\n",
    "claim_per_policy_df['AvgClaimAmount'] = claim_df_grouped[['ClaimedAmount']].mean()\n",
    "claim_per_policy_df['AvgPaidAmount'] = claim_df_grouped[['PaidAmount']].mean()\n",
    "claim_per_policy_df['AvgCustomerPaidAmount'] = claim_df_grouped[['CustomerPaidAmount']].mean()\n",
    "claim_per_policy_df['stdClaimAmount'] = claim_df_grouped[['ClaimedAmount']].std()\n",
    "claim_per_policy_df['stdPaidAmount'] = claim_df_grouped[['PaidAmount']].std()\n",
    "claim_per_policy_df['stdCustomerPaidAmount'] = claim_df_grouped[['CustomerPaidAmount']].std()\n",
    "claim_per_policy_df['VarianceClaimAmount'] = claim_df_grouped[['ClaimedAmount']].var()\n",
    "claim_per_policy_df['VariancePaidAmount'] = claim_df_grouped[['PaidAmount']].var()\n",
    "claim_per_policy_df['VarianceCustomerPaidAmount'] = claim_df_grouped[['CustomerPaidAmount']].var()\n",
    "claim_per_policy_df['maxClaimAmount'] = claim_df_grouped[['ClaimedAmount']].max()\n",
    "claim_per_policy_df['minClaimAmount'] = claim_df_grouped[['ClaimedAmount']].min()\n",
    "claim_per_policy_df['medianClaimAmount'] = claim_df_grouped[['ClaimedAmount']].median()\n",
    "claim_per_policy_df['maxCustomerPaidAmount'] = claim_df_grouped[['ClaimedAmount']].max()\n",
    "claim_per_policy_df['minCustomerPaidAmount'] = claim_df_grouped[['ClaimedAmount']].min()\n",
    "claim_per_policy_df['medianCustomerPaidAmount'] = claim_df_grouped[['ClaimedAmount']].median()\n",
    "\n",
    "policy_df = policy_df.join(claim_per_policy_df, on=['PolicyId'])\n",
    "\n",
    "# Last seen Date: CancelDate if not null else '2016-12-31' (Last date in the data set)\n",
    "policy_df['LastSeenDate'] = policy_df['CancelDate']\n",
    "policy_df['LastSeenDate'].fillna(pd.to_datetime('2016-12-31'), inplace=True)\n",
    "\n",
    "# Date conversions\n",
    "policy_df['EnrollDate'] = pd.to_datetime(policy_df['EnrollDate'])\n",
    "policy_df['CancelDate'] = pd.to_datetime(policy_df['CancelDate'])\n",
    "policy_df['LastSeenDate'] = pd.to_datetime(policy_df['LastSeenDate'])\n",
    "\n",
    "# Length of Stay \"LOS\"\n",
    "policy_df['LOS'] = ((policy_df['LastSeenDate'] - policy_df['EnrollDate'])/np.timedelta64(1, 'M'))\n",
    "policy_df = policy_df.round(decimals=2)\n",
    "\n",
    "# Insurance Status: Active means 1 else 0\n",
    "policy_df['InsuranceStatus'] = policy_df['CancelDate'].apply(lambda x: 0 if pd.notnull(x) else 1)\n",
    "\n",
    "# drop null values\n",
    "policy_df.dropna(axis='index', subset=['MonthlyPremium', 'ClaimedAmount', 'PaidAmount'], inplace=True)\n",
    "\n",
    "# If no claims were made then all amounts are equal to 0 instead of NULL\n",
    "for column in policy_df.columns:\n",
    "    policy_df[column].fillna(0, inplace=True)\n",
    "\n",
    "# display(claim_df.head())\n",
    "display(policy_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polices 31526\n",
      "NC:C =  0.8917401509864873 : 0.10825984901351265\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                   # Data Description #\n",
    "Churn --> 'C' \n",
    "Non-Churn --> 'NC'\n",
    "\"\"\"\n",
    "total_policies = policy_df.shape[0]\n",
    "curr_NC = policy_df[policy_df['InsuranceStatus']==1].shape[0]/float(total_policies)\n",
    "curr_C = policy_df[policy_df['InsuranceStatus']==0].shape[0]/float(total_policies)\n",
    "print(\"Total polices\", total_policies)\n",
    "print(\"NC:C = \",curr_NC, \":\", curr_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sampling Data\n",
    "churn_indices = policy_df[policy_df['InsuranceStatus']==0].index.values\n",
    "nonchurn_indices = policy_df[policy_df['InsuranceStatus']==1].index.values\n",
    "# print(len(churn_indices), len(nonchurn_indices))\n",
    "\n",
    "def sample_train_data(num_samples=None, churn_percent=None):\n",
    "    \"\"\"\n",
    "    :param num_samples: total number of samples to return\n",
    "    :param nc: ratio of non-churn in the return data set\n",
    "    :param c: ration of churn in the return data set\n",
    "    \"\"\"\n",
    "    # check to see if nc + c = 1\n",
    "    churn_sample_space = churn_indices\n",
    "    non_churn_sample_space = nonchurn_indices\n",
    "        \n",
    "    if num_samples is None and churn_percent is None:\n",
    "        # use max samples possible (only 80% for train)\n",
    "        churn_percent = curr_C \n",
    "        num_samples = int(len(churn_indices)*0.8/churn_percent)\n",
    "    elif num_samples is None:\n",
    "        num_samples = int(len(churn_indices)*0.8/churn_percent)\n",
    "    \n",
    "    if churn_percent==None:\n",
    "        # default rate\n",
    "        churn_percent = curr_C \n",
    "    elif churn_percent > 1:\n",
    "        raise ValueError('Churn percent cannot be > 1')\n",
    "    \n",
    "    non_churn_percent = 1 - churn_percent\n",
    "    nc_samples_count = int(num_samples * non_churn_percent)\n",
    "    c_samples_count = int(num_samples * churn_percent)\n",
    "    \n",
    "    \n",
    "    # prune train indices if looking for test set\n",
    "    churn_sample_indices = np.random.choice(churn_sample_space, size=c_samples_count, replace=False)\n",
    "    nonchurn_sample_indices = np.random.choice(non_churn_sample_space, size=nc_samples_count, replace=False)\n",
    "    sample_indices = np.concatenate((churn_sample_indices, nonchurn_sample_indices))\n",
    "    # shuffle\n",
    "    np.random.shuffle(sample_indices)\n",
    "    sample = policy_df.loc[sample_indices, :]\n",
    "    return sample\n",
    "\n",
    "def sample_test_data(skip_indices=[]):\n",
    "    \n",
    "    churn_sample_space = np.setdiff1d(churn_indices, skip_indices)\n",
    "    non_churn_sample_space = np.setdiff1d(nonchurn_indices, skip_indices)\n",
    "    churn_sample_indices = churn_sample_space\n",
    "    nonchurn_sample_indices = np.random.choice(non_churn_sample_space, size=len(churn_sample_indices), replace=False)\n",
    "    sample_indices = np.concatenate((churn_sample_indices, nonchurn_sample_indices))\n",
    "    # shuffle\n",
    "    np.random.shuffle(sample_indices)\n",
    "    sample = policy_df.loc[sample_indices, :]\n",
    "    return sample\n",
    "    \n",
    "# test case\n",
    "test = True\n",
    "if test == False:\n",
    "#     sample = sample_data(100, 0.8, 0.2)\n",
    "    sample = sample_data()\n",
    "    sample.head()\n",
    "    sample_total_policies = sample.shape[0]\n",
    "    NC = sample[sample['InsuranceStatus']==1].shape[0]/float(sample_total_policies)\n",
    "    C = sample[sample['InsuranceStatus']==0].shape[0]/float(sample_total_policies)\n",
    "    print(\"Total polices\", total_policies)\n",
    "    print(\"NC:C = \",NC, \":\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data set 1: contains NC and C in the same ratio as the given in the original data and num_samples = 0.8%\n",
    "train_set1_df = sample_train_data()\n",
    "test_set1_df = sample_test_data(skip_indices=train_set1_df.index.values)\n",
    "\n",
    "# # data set 2: NC:C = 1:1 and contains max samples possible\n",
    "train_set2_df = sample_train_data(churn_percent=0.5)\n",
    "test_set2_df = sample_test_data(skip_indices=train_set2_df.index.values)\n",
    "\n",
    "\n",
    "# # data set 3: NC:C = 1:2 and contains max samples possible\n",
    "train_set3_df = sample_train_data(churn_percent=0.66)\n",
    "test_set3_df = sample_test_data(skip_indices=train_set3_df.index.values)\n",
    "\n",
    "# # data set 4: NC:C = 2:1 and contains max samples possible\n",
    "train_set4_df = sample_train_data(churn_percent=0.33)\n",
    "test_set4_df = sample_test_data(skip_indices=train_set4_df.index.values)\n",
    "\n",
    "# # data set 5: NC:C = 4:1 and contains max samples possible\n",
    "train_set5_df = sample_train_data(churn_percent=0.20)\n",
    "test_set5_df = sample_test_data(skip_indices=train_set5_df.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_columns = list(\n",
    "    set(policy_df.columns) - set(['PolicyId', 'EnrollDate', 'CancelDate', 'InsuranceStatus', 'LastSeenDate',\n",
    "#                                  'ClaimedAmount', 'PaidAmount', 'CustomerPaidAmount' , \n",
    "#                                  'AvgClaimAmount', 'AvgPaidAmount', 'stdClaimAmount', 'stdPaidAmount'\n",
    "#                                   'minClaimAmount', 'maxClaimAmount', 'medianClaimAmount'\n",
    "                                 ]))\n",
    "\n",
    "datasets  = [\n",
    "        [train_set1_df, test_set1_df],\n",
    "        [train_set2_df, test_set2_df],\n",
    "        [train_set3_df, test_set3_df],\n",
    "        [train_set4_df, test_set4_df],\n",
    "        [train_set5_df, test_set5_df]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Data set : 1\n",
      "\t Weight  1\n",
      "\t Test accuracy 0.500732064422\n",
      "train Data set : 2\n",
      "\t Weight  1\n",
      "\t Test accuracy 0.588579795022\n",
      "train Data set : 3\n",
      "\t Weight  1\n",
      "\t Test accuracy 0.550438596491\n",
      "train Data set : 4\n",
      "\t Weight  1\n",
      "\t Test accuracy 0.538067349927\n",
      "train Data set : 5\n",
      "\t Weight  1\n",
      "\t Test accuracy 0.505856515373\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                                        # RF Model #\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "i = 1\n",
    "for dataset in datasets:\n",
    "    print(\"train Data set :\", i)\n",
    "    i += 1\n",
    "    X_train = dataset[0][select_columns]\n",
    "    X_test = dataset[1][select_columns]\n",
    "    y_train = dataset[0][['InsuranceStatus']]\n",
    "    y_test = dataset[1][['InsuranceStatus']]\n",
    "    X_train, X_test = scaler.fit_transform(X_train), scaler.fit_transform(X_test)\n",
    "    for weight in [1]:\n",
    "        model = RF(n_estimators=100, class_weight={0:2,1:1})\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predicted = model.predict(X_test)\n",
    "        print(\"\\t Weight \", weight)\n",
    "#         print(\"\\t Train\",model.score(X_train, y_train))\n",
    "        print(\"\\t Test accuracy\",model.score(X_test, y_test))\n",
    "#         print(confusion_matrix(y_test, y_predicted))\n",
    "#         X2 = sm.add_constant(X_train)\n",
    "#         est = sm.Logit(y_train, X2)\n",
    "#         est2 = est.fit()\n",
    "#         print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Data set : 1\n",
      "\t Weight on disenrolled polcies 0.5\n",
      "\t Test Accuracy 0.5\n",
      "\t Weight on disenrolled polcies 0.75\n",
      "\t Test Accuracy 0.500732064422\n",
      "\t Weight on disenrolled polcies 1\n",
      "\t Test Accuracy 0.501464128843\n",
      "\t Weight on disenrolled polcies 1.5\n",
      "\t Test Accuracy 0.501464128843\n",
      "\t Weight on disenrolled polcies 2\n",
      "\t Test Accuracy 0.501464128843\n",
      "\t Weight on disenrolled polcies 3\n",
      "\t Test Accuracy 0.502928257687\n",
      "\t Weight on disenrolled polcies 4\n",
      "\t Test Accuracy 0.505124450952\n",
      "train Data set : 2\n",
      "\t Weight on disenrolled polcies 0.5\n",
      "\t Test Accuracy 0.507320644217\n",
      "\t Weight on disenrolled polcies 0.75\n",
      "\t Test Accuracy 0.53953147877\n",
      "\t Weight on disenrolled polcies 1\n",
      "\t Test Accuracy 0.576134699854\n",
      "\t Weight on disenrolled polcies 1.5\n",
      "\t Test Accuracy 0.500732064422\n",
      "\t Weight on disenrolled polcies 2\n",
      "\t Test Accuracy 0.494875549048\n",
      "\t Weight on disenrolled polcies 3\n",
      "\t Test Accuracy 0.5\n",
      "\t Weight on disenrolled polcies 4\n",
      "\t Test Accuracy 0.5\n",
      "train Data set : 3\n",
      "\t Weight on disenrolled polcies 0.5\n",
      "\t Test Accuracy 0.581140350877\n",
      "\t Weight on disenrolled polcies 0.75\n",
      "\t Test Accuracy 0.508771929825\n",
      "\t Weight on disenrolled polcies 1\n",
      "\t Test Accuracy 0.5\n",
      "\t Weight on disenrolled polcies 1.5\n",
      "\t Test Accuracy 0.499269005848\n",
      "\t Weight on disenrolled polcies 2\n",
      "\t Test Accuracy 0.499269005848\n",
      "\t Weight on disenrolled polcies 3\n",
      "\t Test Accuracy 0.5\n",
      "\t Weight on disenrolled polcies 4\n",
      "\t Test Accuracy 0.5\n",
      "train Data set : 4\n",
      "\t Weight on disenrolled polcies 0.5\n",
      "\t Test Accuracy 0.502196193265\n",
      "\t Weight on disenrolled polcies 0.75\n",
      "\t Test Accuracy 0.502928257687\n",
      "\t Weight on disenrolled polcies 1\n",
      "\t Test Accuracy 0.505856515373\n",
      "\t Weight on disenrolled polcies 1.5\n",
      "\t Test Accuracy 0.528550512445\n",
      "\t Weight on disenrolled polcies 2\n",
      "\t Test Accuracy 0.547584187408\n",
      "\t Weight on disenrolled polcies 3\n",
      "\t Test Accuracy 0.498535871157\n",
      "\t Weight on disenrolled polcies 4\n",
      "\t Test Accuracy 0.492679355783\n",
      "train Data set : 5\n",
      "\t Weight on disenrolled polcies 0.5\n",
      "\t Test Accuracy 0.5\n",
      "\t Weight on disenrolled polcies 0.75\n",
      "\t Test Accuracy 0.500732064422\n",
      "\t Weight on disenrolled polcies 1\n",
      "\t Test Accuracy 0.501464128843\n",
      "\t Weight on disenrolled polcies 1.5\n",
      "\t Test Accuracy 0.505124450952\n",
      "\t Weight on disenrolled polcies 2\n",
      "\t Test Accuracy 0.505124450952\n",
      "\t Weight on disenrolled polcies 3\n",
      "\t Test Accuracy 0.523426061493\n",
      "\t Weight on disenrolled polcies 4\n",
      "\t Test Accuracy 0.55710102489\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                                        # Logistic Model #\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "i = 1\n",
    "for dataset in datasets:\n",
    "    print(\"train Data set :\", i)\n",
    "    i += 1\n",
    "    X_train = dataset[0][select_columns]\n",
    "    X_test = dataset[1][select_columns]\n",
    "    y_train = dataset[0][['InsuranceStatus']]\n",
    "    y_test = dataset[1][['InsuranceStatus']]\n",
    "    X_train, X_test = scaler.fit_transform(X_train), scaler.fit_transform(X_test)\n",
    "    for weight in [0.5, 0.75, 1, 1.5, 2, 3, 4]:\n",
    "        point_weights = dataset[0].InsuranceStatus.apply(lambda x: weight if x == 0 else 1)\n",
    "        model = LogisticRegression(penalty='l1',)\n",
    "        model.fit(X_train, y_train, point_weights)\n",
    "        y_predicted = model.predict(X_test)\n",
    "        print(\"\\t Weight on disenrolled polcies\", weight)\n",
    "        print(\"\\t Test Accuracy\",model.score(X_test, y_test))\n",
    "        print(\"\",confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Data set : 1\n",
      "\t Test 0.502928257687\n",
      "train Data set : 2\n",
      "\t Test 0.599560761347\n",
      "train Data set : 3\n",
      "\t Test 0.523391812865\n",
      "train Data set : 4\n",
      "\t Test 0.568814055637\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                                        # neural nets Model #\n",
    "\"\"\"\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "datasets  = [\n",
    "        [train_set1_df, test_set1_df],\n",
    "        [train_set2_df, test_set2_df],\n",
    "        [train_set3_df, test_set3_df],\n",
    "        [train_set4_df, test_set4_df]\n",
    "        ]\n",
    "\n",
    "i = 1\n",
    "for dataset in datasets:\n",
    "    print(\"train Data set :\", i)\n",
    "    i += 1\n",
    "    X_train = dataset[0][select_columns]\n",
    "    X_test = dataset[1][select_columns]\n",
    "    y_train = dataset[0][['InsuranceStatus']]\n",
    "    y_test = dataset[1][['InsuranceStatus']]\n",
    "    X_train, X_test = scaler.fit_transform(X_train), scaler.fit_transform(X_test)\n",
    "    model = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(16, 32, 24, 8), random_state=1, \n",
    "                         activation='relu')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "#         print(\"\\t Train\",model.score(X_train, y_train))\n",
    "    print(\"\\t Test\",model.score(X_test, y_test))\n",
    "#     print(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31526"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_df['PolicyId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
