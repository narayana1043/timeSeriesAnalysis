{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PolicyId</th>\n",
       "      <th>EnrollDate</th>\n",
       "      <th>CancelDate</th>\n",
       "      <th>MonthlyPremium</th>\n",
       "      <th>ClaimedAmount</th>\n",
       "      <th>PaidAmount</th>\n",
       "      <th>CustomerPaidAmount</th>\n",
       "      <th>ClaimsCount</th>\n",
       "      <th>AvgCustomerPaidAmount</th>\n",
       "      <th>maxClaimAmount</th>\n",
       "      <th>maxCustomerPaidAmount</th>\n",
       "      <th>LastSeenDate</th>\n",
       "      <th>LOS</th>\n",
       "      <th>InsuranceStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92597</td>\n",
       "      <td>2010-12-07</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>34.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>72.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92808</td>\n",
       "      <td>2010-11-09</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>18.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>73.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93090</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>23.53</td>\n",
       "      <td>762.49</td>\n",
       "      <td>329.46</td>\n",
       "      <td>433.03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>144.34</td>\n",
       "      <td>510.48</td>\n",
       "      <td>510.48</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>73.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93122</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>44.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>73.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93133</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>32.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>73.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93258</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>21.66</td>\n",
       "      <td>33.99</td>\n",
       "      <td>30.60</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>11.33</td>\n",
       "      <td>11.33</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>66.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PolicyId EnrollDate CancelDate  MonthlyPremium  ClaimedAmount  PaidAmount  \\\n",
       "0     92597 2010-12-07 1970-01-01           34.54           0.00        0.00   \n",
       "1     92808 2010-11-09 1970-01-01           18.54           0.00        0.00   \n",
       "2     93090 2010-11-11 1970-01-01           23.53         762.49      329.46   \n",
       "3     93122 2010-11-11 1970-01-01           44.02           0.00        0.00   \n",
       "4     93133 2010-11-11 1970-01-01           32.46           0.00        0.00   \n",
       "5     93258 2010-11-11 2016-05-24           21.66          33.99       30.60   \n",
       "\n",
       "   CustomerPaidAmount  ClaimsCount  AvgCustomerPaidAmount  maxClaimAmount  \\\n",
       "0                0.00          0.0                   0.00            0.00   \n",
       "1                0.00          0.0                   0.00            0.00   \n",
       "2              433.03          3.0                 144.34          510.48   \n",
       "3                0.00          0.0                   0.00            0.00   \n",
       "4                0.00          0.0                   0.00            0.00   \n",
       "5                3.39          3.0                   1.13           11.33   \n",
       "\n",
       "   maxCustomerPaidAmount LastSeenDate    LOS  InsuranceStatus  \n",
       "0                   0.00   2016-12-31  72.81                1  \n",
       "1                   0.00   2016-12-31  73.73                1  \n",
       "2                 510.48   2016-12-31  73.66                1  \n",
       "3                   0.00   2016-12-31  73.66                1  \n",
       "4                   0.00   2016-12-31  73.66                1  \n",
       "5                  11.33   2016-05-24  66.40                0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read data\n",
    "claim_df =  pd.read_csv(\"./data/ClaimLevel.csv\")\n",
    "policy_df = pd.read_csv(\"./data/PolicyLevel.csv\")\n",
    "\n",
    "\"\"\"\n",
    "                        # Data Preparation & Feature generation#\n",
    "Generate a single record per policy that retains max information possible\n",
    "\"\"\"\n",
    "claim_df['CustomerPaidAmount'] = claim_df['ClaimedAmount'] - claim_df['PaidAmount']\n",
    "claim_df_grouped = claim_df.groupby(['PolicyId'])\n",
    "claim_per_policy_df = claim_df_grouped[['ClaimedAmount', 'PaidAmount', 'CustomerPaidAmount']].sum()\n",
    "claim_per_policy_df['ClaimsCount'] = claim_df_grouped.size()\n",
    "# claim_per_policy_df['AvgClaimAmount'] = claim_df_grouped[['ClaimedAmount']].mean()\n",
    "# claim_per_policy_df['AvgPaidAmount'] = claim_df_grouped[['PaidAmount']].mean()\n",
    "claim_per_policy_df['AvgCustomerPaidAmount'] = claim_df_grouped[['CustomerPaidAmount']].mean()\n",
    "# claim_per_policy_df['stdClaimAmount'] = claim_df_grouped[['ClaimedAmount']].std()\n",
    "# claim_per_policy_df['stdPaidAmount'] = claim_df_grouped[['PaidAmount']].std()\n",
    "# claim_per_policy_df['stdCustomerPaidAmount'] = claim_df_grouped[['CustomerPaidAmount']].std()\n",
    "# claim_per_policy_df['VarianceClaimAmount'] = claim_df_grouped[['ClaimedAmount']].var()\n",
    "# claim_per_policy_df['VariancePaidAmount'] = claim_df_grouped[['PaidAmount']].var()\n",
    "# claim_per_policy_df['VarianceCustomerPaidAmount'] = claim_df_grouped[['CustomerPaidAmount']].var()\n",
    "claim_per_policy_df['maxClaimAmount'] = claim_df_grouped[['ClaimedAmount']].max()\n",
    "# claim_per_policy_df['minClaimAmount'] = claim_df_grouped[['ClaimedAmount']].min()\n",
    "# claim_per_policy_df['medianClaimAmount'] = claim_df_grouped[['ClaimedAmount']].median()\n",
    "claim_per_policy_df['maxCustomerPaidAmount'] = claim_df_grouped[['ClaimedAmount']].max()\n",
    "# claim_per_policy_df['minCustomerPaidAmount'] = claim_df_grouped[['ClaimedAmount']].min()\n",
    "# claim_per_policy_df['medianCustomerPaidAmount'] = claim_df_grouped[['ClaimedAmount']].median()\n",
    "\n",
    "policy_df = policy_df.join(claim_per_policy_df, on=['PolicyId'])\n",
    "\n",
    "# Last seen Date: CancelDate if not null else '2016-12-31' (Last date in the data set)\n",
    "policy_df['LastSeenDate'] = policy_df['CancelDate']\n",
    "policy_df['LastSeenDate'].fillna(pd.to_datetime('2016-12-31'), inplace=True)\n",
    "\n",
    "# Date conversions\n",
    "policy_df['EnrollDate'] = pd.to_datetime(policy_df['EnrollDate'])\n",
    "policy_df['CancelDate'] = pd.to_datetime(policy_df['CancelDate'])\n",
    "policy_df['LastSeenDate'] = pd.to_datetime(policy_df['LastSeenDate'])\n",
    "\n",
    "# Length of Stay \"LOS\"\n",
    "policy_df['LOS'] = ((policy_df['LastSeenDate'] - policy_df['EnrollDate'])/np.timedelta64(1, 'M'))\n",
    "policy_df = policy_df.round(decimals=2)\n",
    "\n",
    "# Insurance Status: Active means 1 else 0\n",
    "policy_df['InsuranceStatus'] = policy_df['CancelDate'].apply(lambda x: 0 if pd.notnull(x) else 1)\n",
    "\n",
    "# drop null values\n",
    "policy_df.dropna(axis='index', subset=['MonthlyPremium'], inplace=True)\n",
    "\n",
    "# If no claims were made then all amounts are equal to 0 instead of NULL\n",
    "for column in policy_df.columns:\n",
    "    policy_df[column].fillna(0, inplace=True)\n",
    "\n",
    "# display(claim_df.head())\n",
    "display(policy_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polices 99995\n",
      "NC:C =  0.8678433921696085 : 0.13215660783039151\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                   # Data Description #\n",
    "Churn --> 'C' \n",
    "Non-Churn --> 'NC'\n",
    "\"\"\"\n",
    "total_policies = policy_df.shape[0]\n",
    "curr_NC = policy_df[policy_df['InsuranceStatus']==1].shape[0]/float(total_policies)\n",
    "curr_C = policy_df[policy_df['InsuranceStatus']==0].shape[0]/float(total_policies)\n",
    "print(\"Total polices\", total_policies)\n",
    "print(\"NC:C = \",curr_NC, \":\", curr_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sampling Data\n",
    "churn_indices = policy_df[policy_df['InsuranceStatus']==0].index.values\n",
    "nonchurn_indices = policy_df[policy_df['InsuranceStatus']==1].index.values\n",
    "# print(len(churn_indices), len(nonchurn_indices))\n",
    "\n",
    "def sample_train_data(num_samples=None, churn_percent=None):\n",
    "    \"\"\"\n",
    "    :param num_samples: total number of samples to return\n",
    "    :param nc: ratio of non-churn in the return data set\n",
    "    :param c: ration of churn in the return data set\n",
    "    \"\"\"\n",
    "    # check to see if nc + c = 1\n",
    "    churn_sample_space = churn_indices\n",
    "    non_churn_sample_space = nonchurn_indices\n",
    "        \n",
    "    if num_samples is None and churn_percent is None:\n",
    "        # use max samples possible (only 80% for train)\n",
    "        churn_percent = curr_C \n",
    "        num_samples = int(len(churn_indices)*0.8/churn_percent)\n",
    "    elif num_samples is None:\n",
    "        num_samples = int(len(churn_indices)*0.8/churn_percent)\n",
    "    \n",
    "    if churn_percent==None:\n",
    "        # default rate\n",
    "        churn_percent = curr_C \n",
    "    elif churn_percent > 1:\n",
    "        raise ValueError('Churn percent cannot be > 1')\n",
    "    \n",
    "    non_churn_percent = 1 - churn_percent\n",
    "    nc_samples_count = int(num_samples * non_churn_percent)\n",
    "    c_samples_count = int(num_samples * churn_percent)\n",
    "    \n",
    "    \n",
    "    # prune train indices if looking for test set\n",
    "    churn_sample_indices = np.random.choice(churn_sample_space, size=c_samples_count, replace=False)\n",
    "    nonchurn_sample_indices = np.random.choice(non_churn_sample_space, size=nc_samples_count, replace=False)\n",
    "    sample_indices = np.concatenate((churn_sample_indices, nonchurn_sample_indices))\n",
    "    # shuffle\n",
    "    np.random.shuffle(sample_indices)\n",
    "    sample = policy_df.loc[sample_indices, :]\n",
    "    return sample\n",
    "\n",
    "def sample_test_data(skip_indices=[]):\n",
    "    \n",
    "    churn_sample_space = np.setdiff1d(churn_indices, skip_indices)\n",
    "    non_churn_sample_space = np.setdiff1d(nonchurn_indices, skip_indices)\n",
    "    churn_sample_indices = churn_sample_space\n",
    "    nonchurn_sample_indices = np.random.choice(non_churn_sample_space, size=len(churn_sample_indices), replace=False)\n",
    "    sample_indices = np.concatenate((churn_sample_indices, nonchurn_sample_indices))\n",
    "    # shuffle\n",
    "    np.random.shuffle(sample_indices)\n",
    "    sample = policy_df.loc[sample_indices, :]\n",
    "    return sample\n",
    "    \n",
    "# test case\n",
    "test = True\n",
    "if test == False:\n",
    "#     sample = sample_data(100, 0.8, 0.2)\n",
    "    sample = sample_data()\n",
    "    sample.head()\n",
    "    sample_total_policies = sample.shape[0]\n",
    "    NC = sample[sample['InsuranceStatus']==1].shape[0]/float(sample_total_policies)\n",
    "    C = sample[sample['InsuranceStatus']==0].shape[0]/float(sample_total_policies)\n",
    "    print(\"Total polices\", total_policies)\n",
    "    print(\"NC:C = \",NC, \":\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the train data evaluation this the best set\n",
    "# # data set 2: NC:C = 1:1 and contains max samples possible\n",
    "train_set2_df = sample_train_data(churn_percent=0.5)\n",
    "test_set2_df = sample_test_data(skip_indices=train_set2_df.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_columns = list(\n",
    "    set(policy_df.columns) - set(['PolicyId', 'EnrollDate', 'CancelDate', 'InsuranceStatus', 'LastSeenDate',\n",
    "#                                  'ClaimedAmount', 'PaidAmount', 'CustomerPaidAmount' , \n",
    "#                                  'AvgClaimAmount', 'AvgPaidAmount', 'stdClaimAmount', 'stdPaidAmount'\n",
    "#                                   'minClaimAmount', 'maxClaimAmount', 'medianClaimAmount'\n",
    "                                 ]))\n",
    "\n",
    "datasets  = [\n",
    "        [train_set2_df, test_set2_df],\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.578130911843\n",
      "tn, fp, fn, tp  [1447 1196 1034 1609]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                                        # RF Model #\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "for dataset in datasets:\n",
    "    X_train = dataset[0][select_columns]\n",
    "    X_test = dataset[1][select_columns]\n",
    "    y_train = dataset[0][['InsuranceStatus']]\n",
    "    y_test = dataset[1][['InsuranceStatus']]\n",
    "    X_train, X_test = scaler.fit_transform(X_train), scaler.fit_transform(X_test)\n",
    "    model = RF(n_estimators=100, class_weight={0:3,1:1})\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    print(\"Test accuracy\",model.score(X_test, y_test))\n",
    "    print(\"tn, fp, fn, tp \",confusion_matrix(y_test, y_predicted).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight on disenrolled polcies 0.5\n",
      "Test Accuracy 0.501891789633\n",
      "tn, fp, fn, tp  [  16 2627    6 2637]\n",
      "Weight on disenrolled polcies 0.75\n",
      "Test Accuracy 0.595724555429\n",
      "tn, fp, fn, tp  [1088 1555  582 2061]\n",
      "Weight on disenrolled polcies 1\n",
      "Test Accuracy 0.59515701854\n",
      "tn, fp, fn, tp  [1863  780 1360 1283]\n",
      "Weight on disenrolled polcies 1.5\n",
      "Test Accuracy 0.551645856981\n",
      "tn, fp, fn, tp  [2346  297 2073  570]\n",
      "Weight on disenrolled polcies 2\n",
      "Test Accuracy 0.530457813091\n",
      "tn, fp, fn, tp  [2505  138 2344  299]\n",
      "Weight on disenrolled polcies 3\n",
      "Test Accuracy 0.514566780174\n",
      "tn, fp, fn, tp  [2593   50 2516  127]\n",
      "Weight on disenrolled polcies 4\n",
      "Test Accuracy 0.508891411275\n",
      "tn, fp, fn, tp  [2615   28 2568   75]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                                        # Logistic Model #\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for dataset in datasets:\n",
    "    X_train = dataset[0][select_columns]\n",
    "    X_test = dataset[1][select_columns]\n",
    "    y_train = dataset[0][['InsuranceStatus']]\n",
    "    y_test = dataset[1][['InsuranceStatus']]\n",
    "    X_train, X_test = scaler.fit_transform(X_train), scaler.fit_transform(X_test)\n",
    "    for weight in [0.5, 0.75, 1, 1.5, 2, 3, 4]:\n",
    "        point_weights = dataset[0].InsuranceStatus.apply(lambda x: weight if x == 0 else 1)\n",
    "        model = LogisticRegression(penalty='l1',)\n",
    "        model.fit(X_train, y_train, point_weights)\n",
    "        y_predicted = model.predict(X_test)\n",
    "        print(\"Weight on disenrolled polcies\", weight)\n",
    "        print(\"Test Accuracy\",model.score(X_test, y_test))\n",
    "        print(\"tn, fp, fn, tp \",confusion_matrix(y_test, y_predicted).ravel())\n",
    "#         X2 = sm.add_constant(X_train)\n",
    "#         est = sm.Logit(y_train, X2)\n",
    "#         est2 = est.fit()\n",
    "#         print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0.598751418842\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "                                        # neural nets Model #\n",
    "\"\"\"\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "for dataset in datasets:\n",
    "    X_train = dataset[0][select_columns]\n",
    "    X_test = dataset[1][select_columns]\n",
    "    y_train = dataset[0][['InsuranceStatus']]\n",
    "    y_test = dataset[1][['InsuranceStatus']]\n",
    "    X_train, X_test = scaler.fit_transform(X_train), scaler.fit_transform(X_test)\n",
    "    model = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(16, 32, 24, 8), random_state=1, \n",
    "                         activation='relu')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "#         print(\"\\t Train\",model.score(X_train, y_train))\n",
    "    print(\"Test\",model.score(X_test, y_test))\n",
    "#     print(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
